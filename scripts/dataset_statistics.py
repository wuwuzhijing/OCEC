import csv
import os
from collections import defaultdict
from PIL import Image
import numpy as np
import argparse
import matplotlib.pyplot as plt

# --- é…ç½®å‚æ•° ---
CSV_DIR = '/ssddisk/guochuang/ocec/list_hq'
REPORT_DIR = '/103/guochuang/Code/myOCEC/logs/dataset/list_hq/'
REPORT_FILENAME = 'dataset_stats_report.txt'
REPORT_PATH = os.path.join(REPORT_DIR, REPORT_FILENAME)
PIXEL_SAMPLE_LIMIT = 5000000  # é™åˆ¶åƒç´ é‡‡æ ·æ•°é‡ï¼Œé¿å…å†…å­˜æº¢å‡º

# æƒ³è¦ç»Ÿè®¡çš„ CSV æ–‡ä»¶åˆ—è¡¨ï¼ˆè¯·æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
CSV_FILENAMES = [
    f"annotation_{i:04d}.csv" for i in range(24, 25)] + [
    f"cropped_merged_{i:01d}.csv" for i in range(0, 5)
]

# ===============================================
# A. ç»˜å›¾å‡½æ•°
# ===============================================

def plot_with_stats(data, title, xlabel, path):
    """ç»˜åˆ¶å¸¦ç»Ÿè®¡æ ‡è®°çš„ç›´æ–¹å›¾"""
    if not data:
        return
    
    data = np.array(data)
    mean_val = np.mean(data)
    median_val = np.median(data)
    std_val = np.std(data)

    plt.figure(figsize=(10, 6))
    
    # ç»˜åˆ¶ç›´æ–¹å›¾
    plt.hist(data, bins=50, color='skyblue', edgecolor='black', alpha=0.7)
    
    # æ ‡è®°å‡å€¼ (Mean)
    plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_val:.2f}')
    
    # æ ‡è®°ä¸­ä½æ•° (Median)
    plt.axvline(median_val, color='green', linestyle='solid', linewidth=2, label=f'Median: {median_val:.2f}')
    
    # æ·»åŠ æ ‡å‡†å·®æ–‡å­—
    plt.text(0.95, 0.95, f'Std Dev: {std_val:.2f}', transform=plt.gca().transAxes, 
             fontsize=10, verticalalignment='top', horizontalalignment='right', 
             bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.6))
    
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel('Frequency')
    plt.legend()
    plt.grid(axis='y', alpha=0.5)
    
    plt.savefig(path)
    plt.close()
    print(f"âœ… {title} å›¾å·²ä¿å­˜: {path}")
    return path

def plot_class_distribution(class_counts, path):
    """ç»˜åˆ¶ç±»åˆ«æ•°é‡æŸ±çŠ¶å›¾"""
    if not class_counts:
        return
        
    labels = list(class_counts.keys())
    counts = list(class_counts.values())
    
    plt.figure(figsize=(8, 6))
    bars = plt.bar(labels, counts, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
    
    plt.title('Global Class Distribution')
    plt.xlabel('Class Label')
    plt.ylabel('Sample Count')
    
    # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°é‡
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + len(counts)*100, f'{yval:,}', ha='center', va='bottom', fontsize=9)
        
    plt.grid(axis='y', alpha=0.5)
    
    plt.savefig(path)
    plt.close()
    print(f"âœ… ç±»åˆ«åˆ†å¸ƒå›¾å·²ä¿å­˜: {path}")
    return path

def plot_pixel_distribution(raw_pixels_sample, path):
    """ç»˜åˆ¶åƒç´ å€¼åˆ†å¸ƒç›´æ–¹å›¾ (R/G/B ä¸‰é€šé“)"""
    if len(raw_pixels_sample) == 0:
        return
        
    # å°†é‡‡æ ·çš„åƒç´ æ•°æ®è½¬ä¸º Numpy æ•°ç»„ (Shape: N x 3)
    pixels_array = np.array(raw_pixels_sample)
    
    plt.figure(figsize=(12, 7))
    colors = ['red', 'green', 'blue']
    labels = ['Red Channel', 'Green Channel', 'Blue Channel']
    
    # ç»˜åˆ¶ä¸‰é€šé“çš„ç›´æ–¹å›¾
    for i in range(3):
        # bins=50, èŒƒå›´[0, 1] å› ä¸ºæ•°æ®å·²ç»è¢«å½’ä¸€åŒ–åˆ° [0, 1]
        plt.hist(pixels_array[:, i], bins=50, range=[0, 1], alpha=0.6, color=colors[i], label=labels[i], edgecolor='none')
        
    plt.title('Pixel Value Distribution (Normalized [0, 1])')
    plt.xlabel('Pixel Value')
    plt.ylabel('Frequency')
    plt.legend()
    plt.grid(axis='y', alpha=0.5)
    
    plt.savefig(path)
    plt.close()
    print(f"âœ… åƒç´ åˆ†å¸ƒå›¾å·²ä¿å­˜: {path}")
    return path


# ===============================================
# B. åˆ†æå‡½æ•°
# ===============================================

def analyze_dataset(csv_filenames, check_pixel_stats=False, generate_plots=False):
    # ... (åˆå§‹åŒ–å’Œæ—¥å¿—å‡½æ•°ä¸ V4.0 ç›¸åŒ) ...
    total_samples = 0
    total_missing_files = 0
    global_class_counts = defaultdict(int)
    
    pixel_sum = np.zeros(3)
    pixel_sq_sum = np.zeros(3)
    total_pixels = 0
    
    raw_heights = []
    raw_widths = []
    # æ–°å¢ï¼šç”¨äºåƒç´ åˆ†å¸ƒé‡‡æ ·çš„åˆ—è¡¨
    raw_pixels_sample = [] 
    
    file_analysis_details = {}
    report_buffer = []

    def log_report(message):
        report_buffer.append(message)
        print(message)
    
    log_report(f"--- å¼€å§‹åˆ†æ {len(csv_filenames)} ä¸ª CSV æ–‡ä»¶ ---")
    
    # ... (æ–‡ä»¶éå†å’Œæ•°æ®æ”¶é›†å¾ªç¯) ...
    for filename in csv_filenames:
        csv_path = os.path.join(CSV_DIR, filename)
        if not os.path.exists(csv_path):
            log_report(f"âš ï¸ è­¦å‘Šï¼šæ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡: {csv_path}")
            continue

        log_report(f"\n> æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")
        
        file_samples = 0
        file_class_counts = defaultdict(int)

        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            try:
                next(reader) 
            except StopIteration:
                continue

            for row in reader:
                # ... (è¡Œå¤„ç†å’Œè®¡æ•°é€»è¾‘çœç•¥) ...
                if len(row) != 2:
                    continue
                
                file_path, label = row
                
                total_samples += 1
                global_class_counts[label] += 1
                file_samples += 1
                file_class_counts[label] += 1
                
                if not os.path.exists(file_path):
                    total_missing_files += 1
                    continue
                    
                # å›¾åƒå±æ€§ç»Ÿè®¡ (éœ€è¦è¯»å–å›¾åƒ)
                if check_pixel_stats or generate_plots:
                    try:
                        img = Image.open(file_path).convert('RGB')
                        width, height = img.size
                        img_array = np.array(img, dtype=np.float32) / 255.0
                        
                        # I. åƒç´ ç»Ÿè®¡ (Mean/Std)
                        if check_pixel_stats:
                            total_pixels += height * width
                            pixel_sum += np.sum(img_array, axis=(0, 1))
                            pixel_sq_sum += np.sum(img_array**2, axis=(0, 1))

                        # II. ç»˜å›¾æ•°æ®æ”¶é›†
                        if generate_plots:
                            raw_heights.append(height)
                            raw_widths.append(width)
                            
                            # **åƒç´ é‡‡æ ·é€»è¾‘**ï¼šåªé‡‡æ ·ä¸è¶…è¿‡ PIXEL_SAMPLE_LIMIT çš„æ•°é‡
                            if len(raw_pixels_sample) < PIXEL_SAMPLE_LIMIT:
                                # éšæœºé€‰æ‹©å½“å‰å›¾åƒä¸­çš„åƒç´ ï¼ŒæŒ‰æ¯”ä¾‹é‡‡æ ·
                                flat_pixels = img_array.reshape(-1, 3)
                                sample_size = min(len(flat_pixels), int(PIXEL_SAMPLE_LIMIT / total_samples))
                                
                                # éšæœºé€‰æ‹©ç´¢å¼•
                                indices = np.random.choice(flat_pixels.shape[0], size=sample_size, replace=False)
                                raw_pixels_sample.extend(flat_pixels[indices])
                            
                    except Exception as e:
                        total_missing_files += 1

        if file_samples > 0:
            file_analysis_details[filename] = {'samples': file_samples, 'class_counts': file_class_counts}

    # ... (ç»“æœè®¡ç®—å’Œå†™å…¥ç¼“å†²åŒºé€»è¾‘çœç•¥ï¼Œä¸ V4.0 ç›¸åŒ) ...
    global_mean = pixel_sum / total_pixels if total_pixels > 0 else np.zeros(3)
    global_std = np.sqrt(pixel_sq_sum / total_pixels - global_mean**2) if total_pixels > 0 else np.zeros(3)
    
    # æŠ¥å‘Šä¸»ä½“å†…å®¹å†™å…¥ report_buffer 
    log_report("\n" + "="*70)
    log_report("                 æ•°æ®é›†ç»¼åˆç»Ÿè®¡æŠ¥å‘Š")
    log_report("="*70)
    log_report(f"æ€»æ ·æœ¬æ•° (Total Samples): {total_samples:,}")
    log_report(f"æ–‡ä»¶ç¼ºå¤±/æ— æ•ˆæ•° (Missing/Invalid Files): {total_missing_files:,}")
    # ... (A. æ–‡ä»¶çº§ç±»åˆ«åˆ†å¸ƒ) ...
    log_report("\n--- A. æ–‡ä»¶çº§ç±»åˆ«åˆ†å¸ƒ (Per-File Class Balance) ---")
    for filename, details in file_analysis_details.items():
        counts = details['class_counts']
        output = f" {filename} ({details['samples']:,} rows): "
        if '0' in counts and '1' in counts and counts['0'] > 0 and counts['1'] > 0:
            ratio = counts['0'] / (counts['0'] + counts['1']) * 100
            output += f" 0: {counts['0']:,}, 1: {counts['1']:,} (0ç±»å æ¯”: {ratio:.2f}%)"
        else:
            output += f" åˆ†å¸ƒä¸å®Œæ•´æˆ–å•ç±»åˆ«: {dict(counts)}"
        log_report(output)
    
    # ... (B. å…¨å±€åƒç´ ç»Ÿè®¡) ...
    if check_pixel_stats:
        log_report("\n--- B. å…¨å±€åƒç´ ç»Ÿè®¡ (Normalization Parameters) ---")
        log_report(f"å›¾åƒé€šé“æ•°: {3} (é»˜è®¤ä¸º RGB)")
        log_report(f"å…¨å±€å‡å€¼ (Mean, R/G/B): {global_mean}")
        log_report(f"å…¨å±€æ ‡å‡†å·® (Std Dev, R/G/B): {global_std}")
        log_report("\nğŸ’¡ å»ºè®®ï¼šå°†è¿™äº›å€¼ç”¨äºæ‚¨çš„ DataLoader/Transforms é…ç½®ä¸­ã€‚")
    else:
        log_report("\nğŸ’¡ åƒç´ ç»Ÿè®¡æœªè¿è¡Œã€‚")

    # ... (C. æ€»ä½“ç±»åˆ«åˆ†å¸ƒ) ...
    log_report("\n--- C. æ€»ä½“ç±»åˆ«åˆ†å¸ƒ (Global Class Distribution) ---")
    sorted_counts = sorted(global_class_counts.items(), key=lambda item: item[1], reverse=True)
    if total_samples > 0:
        for label, count in sorted_counts:
            percentage = (count / total_samples) * 100
            log_report(f"ç±»åˆ« {label}: {count:,} ({percentage:.2f}%)")
    log_report("="*70)
    
    # --- æœ€ç»ˆå†™å…¥æ–‡ä»¶ ---
    if not os.path.exists(REPORT_DIR):
        os.makedirs(REPORT_DIR)
        
    final_report_content = []
    # ... (å†™å…¥ CSV åˆ—è¡¨å’ŒæŠ¥å‘Šä¸»ä½“é€»è¾‘çœç•¥ï¼Œä¸ V4.0 ç›¸åŒ) ...
    final_report_content.append("="*70)
    final_report_content.append(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {os.popen('date').read().strip()}")
    final_report_content.append(f"æŠ¥å‘Šæ–‡ä»¶è·¯å¾„: {REPORT_PATH}")
    final_report_content.append("\n--- åˆ†æçš„ CSV æ–‡ä»¶åˆ—è¡¨ ---")
    for csv_file in csv_filenames:
        final_report_content.append(f"- {csv_file}")
    final_report_content.append("--- æŠ¥å‘Šä¸»ä½“ ---")
    final_report_content.extend(report_buffer)

    with open(REPORT_PATH, 'w', encoding='utf-8') as f:
        f.write('\n'.join(final_report_content))
        
    print(f"\nâœ… ç»Ÿè®¡æ•°æ®å·²å†™å…¥æŠ¥å‘Š: {REPORT_PATH}")
    
    # ç»˜å›¾è¿”å›
    if generate_plots:
        return raw_heights, raw_widths, global_class_counts, raw_pixels_sample
    else:
        return None, None, None, None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ•°æ®é›† CSV æ–‡ä»¶ç»Ÿè®¡å·¥å…· (V5.0 - å…¨é¢å¯è§†åŒ–)")
    parser.add_argument(
        '--check_pixel_stats', 
        action='store_true', 
        help="å¯ç”¨è€—æ—¶çš„åƒç´ å‡å€¼å’Œæ ‡å‡†å·®è®¡ç®—ã€‚"
    )
    parser.add_argument(
        '--generate_plots', 
        action='store_true', 
        help="ç”Ÿæˆå›¾åƒå°ºå¯¸ã€ç±»åˆ«å’Œåƒç´ åˆ†å¸ƒå›¾è¡¨ï¼ˆéœ€è¦è¯»å–å›¾åƒæ–‡ä»¶ï¼‰ã€‚"
    )
    args = parser.parse_args()
    
    read_images = args.check_pixel_stats or args.generate_plots
    
    heights, widths, class_counts, pixels = analyze_dataset(CSV_FILENAMES, read_images, args.generate_plots)
    
    if args.generate_plots:
        plot_results = []
        plot_results.append(plot_with_stats(heights, 'Distribution of Image Heights (with Stats)', 'Height (Pixels)', os.path.join(REPORT_DIR, 'height_stats_histogram.png')))
        plot_results.append(plot_with_stats(widths, 'Distribution of Image Widths (with Stats)', 'Width (Pixels)', os.path.join(REPORT_DIR, 'width_stats_histogram.png')))
        plot_results.append(plot_class_distribution(class_counts, os.path.join(REPORT_DIR, 'class_distribution_bar.png')))
        plot_results.append(plot_pixel_distribution(pixels, os.path.join(REPORT_DIR, 'pixel_distribution_histogram.png')))
        
        print("\n--- æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆ ---")
        for path in plot_results:
            if path:
                print(f"ğŸ–¼ï¸ {os.path.basename(path)}")